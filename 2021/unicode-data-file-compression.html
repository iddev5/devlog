<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Unicode data file compression: achieving 40-70% reduction over gzip alone</title>
  <meta name="description" content="A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it’s sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/2021/unicode-data-file-compression">
  
  
  <link rel="alternate" type="application/rss+xml" title="Hexops&#39; devlog" href="/feed.xml">

  

  
  <meta property="og:title" content="Unicode data file compression: achieving 40-70% reduction over gzip alone">
  <meta property="og:site_name" content="Hexops&#39; devlog">
  <meta property="og:url" content="/2021/unicode-data-file-compression">
  <meta property="og:description" content="A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it’s sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression.">
  
  
    <meta property="og:image" content="https://raw.githubusercontent.com/hexops/website/master/media/png/square_logo.png">
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Unicode data file compression: achieving 40-70% reduction over gzip...">
  <meta name="twitter:description" content="A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it’s sometimes worth giving it a shot, and how to tell when you should t...">
  
  
    <meta name="twitter:image:src" content="https://raw.githubusercontent.com/hexops/website/master/media/png/square_logo.png">
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

    <div class="wrapper">

      <a class="site-title" href="/">
        <img alt="Hexops" class="logo color" src="https://raw.githubusercontent.com/hexops/media/main/logo.svg" style="height: 30px;">' devlog
      </a>
      <link href="/assets/font/stylesheet.css?v2" rel="stylesheet">
      <link rel="icon" sizes="any" type="image/svg+xml" href="https://raw.githubusercontent.com/hexops/website/master/media/svg/icon.svg">

      <script async defer data-domain="devlog.hexops.com" src="https://devlog.hexops.com/assets/opendata.js"></script>

      <nav class="site-nav">
        
          
          <a class="page-link" href="/about">About</a>
        
          
          <a class="page-link" href="/archives">Archives</a>
        
          
          <a class="page-link" href="https://github.com/hexops">GitHub</a>
        
      </nav>
  
    </div>
  
  </header>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Unicode data file compression: achieving 40-70% reduction over gzip alone</h1>
    
    <p class="post-meta"><time datetime="2021-07-03T00:00:00+00:00" itemprop="datePublished">Jul 3, 2021</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Stephen Gutekanst</span></span> •
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/categories/unicode/">unicode,</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/categories/compression/">compression</a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it’s sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression.</p>

<ul>
  <li><a href="#background">Background</a></li>
  <li><a href="#problem">Problem</a></li>
  <li><a href="#investigation">Investigation</a>
    <ul>
      <li><a href="#binary-encoding">Binary encoding?</a></li>
      <li><a href="#differential-encodingcompression">Differential encoding/compression?</a></li>
      <li><a href="#go-implementation">Go implementation</a></li>
    </ul>
  </li>
  <li><a href="#zig-implementation">Zig implementation</a>
    <ul>
      <li><a href="#differential-encoding-state-machine">Differential encoding state machine</a></li>
      <li><a href="#a-stream-of-op-codes">A stream of op codes</a></li>
      <li><a href="#iteratively-finding-the-most-lucrative-opcodes">Iteratively finding the most lucrative opcodes</a></li>
      <li><a href="#a-stream-of-opcodes-for-a-state-machine-a-natural-progression-from-a-binary-format">A stream of opcodes for a state machine: a natural progression from a binary format?</a></li>
    </ul>
  </li>
  <li><a href="#results-better-than-gzipbrotli-and-even-better-with-them">Results? Better than gzip/brotli; and even better <em>with</em> them!</a>
    <ul>
      <li><a href="#why-test-with-gzipbrotli-but-not-others">Why test with gzip/brotli but not others?</a></li>
      <li><a href="#how-complex-is-the-implementation">How complex is the implementation?</a></li>
    </ul>
  </li>
  <li><a href="#notable-mention">Notable mention</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="background">Background</h2>

<p>Two weeks ago, I began using <a href="https://github.com/jecolon/ziglyph">Ziglyph</a> (“Unicode processing with Zig, and a UTF-8 string type: Zigstr.”) - an awesome library by <a href="https://github.com/jecolon">@jecolon</a>, for grapheme cluster sorting in <a href="https://github.com/hexops/zorex">Zorex, an omnipotent regexp engine</a>.</p>

<p>I don’t personally have any prior experience working with the lower level details of Unicode, or compression algorithms for that matter.</p>

<h2 id="problem">Problem</h2>

<p>As I stumbled into the wondrous world that is Unicode text sorting (see also my article: <a href="2021-06-27-unicode-sorting-why-browsers-added-special-emoji-matching.md">Unicode sorting is hard &amp; why browsers added special emoji matching to regexp</a>) and began using Ziglyph, I came across an issue: the standard Unicode collation algorithm, which Ziglyph implements, depends on some large Unicode data tables for normalization and sort keys - even gzipped these were fairly large:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hexops-mac:zorex slimsag<span class="nv">$ </span><span class="nb">du</span> <span class="nt">-sh</span> asset/<span class="k">*</span>
308K	asset/uca-allkeys.txt.gz
260K	asset/ucd-UnicodeData.txt.gz
</code></pre></div></div>

<p>These file sizes may seem small, but one of my goals is to make Zorex a real competitor to e.g. a browser’s native regexp engine. That’s challenging because WebAssembly bundle sizes matter <em>a lot</em> in that context, and using the browser’s regexp implementation is virtually free.</p>

<h2 id="investigation">Investigation</h2>

<p>I set out to try and reduce the size of these data files. First I <a href="https://github.com/jecolon/ziglyph/issues/3">opened an issue and asked</a> if anyone else had thoughts around reducing the size of this data. The author of Ziglyph <a href="https://github.com/jecolon">@jecolon</a> is awesome and readily had some ideas and was able to reduce the two files substantially by removing unnecessary data (such as comments, etc.)</p>

<p>Curious how much further we could go, I kept squinting at the data files (warning: large):</p>

<ul>
  <li>http://www.unicode.org/Public/UCA/latest/allkeys.txt</li>
  <li>http://www.unicode.org/Public/UNIDATA/UnicodeData.txt</li>
</ul>

<h3 id="binary-encoding">Binary encoding?</h3>

<p>My first thoughts were that a binary encoding would likely reduce the size a lot. I pulled in some help from Hobbyist reverse engineer <a href="https://github.com/Andoryuuta">@Andoryuuta</a> and he got started on a binary encoding for UnicodeData.txt based on the spec. With that, he was able to reduce the original 1.9M allkeys.txt file down to 250K (125K gzipped) - quite a win.</p>

<h3 id="differential-encodingcompression">Differential encoding/compression?</h3>

<p>My secondary thought was that, scrolling through these data files it was obvious most entries were derived from prior entries. Many entries were long runs of data where the next entry had the same value, plus a small increment. For example, at the start of the <code class="language-plaintext highlighter-rouge">allkeys.txt</code> file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0000  ; [.0000.0000.0000] # NULL (in ISO 6429)
0001  ; [.0000.0000.0000] # START OF HEADING (in ISO 6429)
0002  ; [.0000.0000.0000] # START OF TEXT (in ISO 6429)
0003  ; [.0000.0000.0000] # END OF TEXT (in ISO 6429)
0004  ; [.0000.0000.0000] # END OF TRANSMISSION (in ISO 6429)
0005  ; [.0000.0000.0000] # ENQUIRY (in ISO 6429)
0006  ; [.0000.0000.0000] # ACKNOWLEDGE (in ISO 6429)
0007  ; [.0000.0000.0000] # BELL (in ISO 6429)
0008  ; [.0000.0000.0000] # BACKSPACE (in ISO 6429)
000E  ; [.0000.0000.0000] # SHIFT OUT (in ISO 6429)
000F  ; [.0000.0000.0000] # SHIFT IN (in ISO 6429)
</code></pre></div></div>

<p>Of course, not all sections are so sequential. Many sections are a bit more arbitrary:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FF9A  ; [.4304.0020.0012] # HALFWIDTH KATAKANA LETTER RE
32F9  ; [.4304.0020.0013] # CIRCLED KATAKANA RE
3355  ; [.4304.0020.001C][.42FB.0020.001C] # SQUARE REMU
3356  ; [.4304.0020.001C][.430A.0020.001C][.42EE.0020.001C][.42E3.0020.001C][.0000.0037.001C][.430A.0020.001C] # SQUARE RENTOGEN
308D  ; [.4305.0020.000E] # HIRAGANA LETTER RO
31FF  ; [.4305.0020.000F] # KATAKANA LETTER SMALL RO
30ED  ; [.4305.0020.0011] # KATAKANA LETTER RO
</code></pre></div></div>

<p>Still, there are obvious patterns one can see in the way these values change.</p>

<h3 id="go-implementation">Go implementation</h3>

<p>I did a quick hacky Go implementation of differential encoding on these files to see how well that would work. The results were pretty good, and already beat just <code class="language-plaintext highlighter-rouge">gzip -9</code> compression of the files:</p>

<table>
  <thead>
    <tr>
      <th>File</th>
      <th>Original</th>
      <th>Original + <code class="language-plaintext highlighter-rouge">gzip -9</code></th>
      <th>My compression</th>
      <th>My compression + <code class="language-plaintext highlighter-rouge">gzip -9</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Decompositions.txt</td>
      <td>72K</td>
      <td>28K</td>
      <td>48K</td>
      <td>12K</td>
    </tr>
    <tr>
      <td>allkeys-minimal.txt</td>
      <td>500K</td>
      <td>148K</td>
      <td>204K</td>
      <td>36K</td>
    </tr>
  </tbody>
</table>

<p>However, because I chose to do these experiments in Go I found a number of inefficiencies:</p>

<ul>
  <li>There were a lot of locations where I encoded things as 8-bit unsigned integers (Go’s smallest value type) instead of a more optimal 4-bit unsigned integer. I could’ve done bit shifting, but it would’ve been annoying.</li>
  <li>There were also many places where I encoded Unicode codepoints as 32-bit unsigned integers, rather than a more optimal 21-bit unsigned integer (because valid Unicode codepoints do not exceed that range.)</li>
</ul>

<p>For a real implementation, I switched over to Zig.</p>

<h2 id="zig-implementation">Zig implementation</h2>

<p>Actually, two things made working on this in Zig much easier than in Go:</p>

<ol>
  <li>Zig has variable bit-width integers: I could just write <code class="language-plaintext highlighter-rouge">u4</code> and <code class="language-plaintext highlighter-rouge">u21</code> values instead of needing to handle bit packing within larger size integers myself. That was <em>nice</em>.</li>
  <li>In the Zig standard library it provides:</li>
</ol>

<ul>
  <li><a href="https://sourcegraph.com/github.com/ziglang/zig@0.8.0/-/blob/lib/std/io/bit_writer.zig?L152-202"><code class="language-plaintext highlighter-rouge">std.io.BitWriter</code></a></li>
  <li><a href="https://sourcegraph.com/github.com/ziglang/zig@0.8.0/-/blob/lib/std/io/bit_reader.zig?L176-248"><code class="language-plaintext highlighter-rouge">std.io.BitReader</code></a></li>
</ul>

<p>With these two features, it became incredibly easy to write the most optimal bit-packed encoding of the data.</p>

<p>In fact, the basic uncompressed binary format <a href="https://github.com/jecolon/ziglyph/pull/7/commits/7d4042d8df21cc11eaf42177c2f4d9b3afd9c4a7">was only a few lines to encode</a>:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">fn</span> <span class="n">compressTo</span><span class="p">(</span><span class="n">self</span><span class="p">:</span> <span class="o">*</span><span class="n">DecompFile</span><span class="p">,</span> <span class="n">writer</span><span class="p">:</span> <span class="n">anytype</span><span class="p">)</span> <span class="o">!</span><span class="k">void</span> <span class="p">{</span>
    <span class="k">var</span> <span class="n">buf_writer</span> <span class="o">=</span> <span class="n">std</span><span class="p">.</span><span class="py">io</span><span class="p">.</span><span class="nf">bufferedWriter</span><span class="p">(</span><span class="n">writer</span><span class="p">);</span>
    <span class="k">var</span> <span class="n">out</span> <span class="o">=</span> <span class="n">std</span><span class="p">.</span><span class="py">io</span><span class="p">.</span><span class="nf">bitWriter</span><span class="p">(.</span><span class="py">Little</span><span class="p">,</span> <span class="n">buf_writer</span><span class="p">.</span><span class="nf">writer</span><span class="p">());</span>
    <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">writeBits</span><span class="p">(</span><span class="nb">@intCast</span><span class="p">(</span><span class="kt">u16</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="py">entries</span><span class="p">.</span><span class="py">items</span><span class="p">.</span><span class="py">len</span><span class="p">),</span> <span class="mi">16</span><span class="p">);</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">next</span><span class="p">())</span> <span class="p">|</span><span class="n">entry</span><span class="p">|</span> <span class="p">{</span>
        <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">writeBits</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="py">key_len</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
        <span class="mi">_</span> <span class="o">=</span> <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="py">key</span><span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="n">entry</span><span class="p">.</span><span class="py">key_len</span><span class="p">]);</span>
        <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">writeBits</span><span class="p">(</span><span class="nb">@enumToInt</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="py">value</span><span class="p">.</span><span class="py">form</span><span class="p">),</span> <span class="nb">@bitSizeOf</span><span class="p">(</span><span class="n">Form</span><span class="p">));</span>
        <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">writeBits</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="py">value</span><span class="p">.</span><span class="py">len</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="py">value</span><span class="p">.</span><span class="py">seq</span><span class="p">[</span><span class="mi">0</span><span class="o">..</span><span class="n">entry</span><span class="p">.</span><span class="py">value</span><span class="p">.</span><span class="py">len</span><span class="p">])</span> <span class="p">|</span><span class="n">s</span><span class="p">|</span> <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">writeBits</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">21</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">try</span> <span class="n">out</span><span class="p">.</span><span class="nf">flushBits</span><span class="p">();</span>
    <span class="k">try</span> <span class="n">buf_writer</span><span class="p">.</span><span class="nf">flush</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="differential-encoding-state-machine">Differential encoding state machine</h3>

<p>To handle the compression, I started out <em>really</em> simple. First I encoded just a binary version of the data with no compression. The most important thing was to get to a point where I could start testing some theories about what would compress the data really well, and validate that it was in fact being losslessly compressed/decompressed without issues via tests:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">test</span> <span class="s">"compression_is_lossless"</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">allocator</span> <span class="o">=</span> <span class="n">testing</span><span class="p">.</span><span class="py">allocator</span><span class="p">;</span>

    <span class="c">// Compress UnicodeData.txt -&gt; Decompositions.bin</span>
    <span class="k">var</span> <span class="n">file</span> <span class="o">=</span> <span class="k">try</span> <span class="n">parseFile</span><span class="p">(</span><span class="n">allocator</span><span class="p">,</span> <span class="s">"src/data/ucd/UnicodeData.txt"</span><span class="p">);</span>
    <span class="k">defer</span> <span class="n">file</span><span class="p">.</span><span class="nf">deinit</span><span class="p">();</span>
    <span class="k">try</span> <span class="n">file</span><span class="p">.</span><span class="nf">compressToFile</span><span class="p">(</span><span class="s">"src/data/ucd/Decompositions.bin"</span><span class="p">);</span>

    <span class="c">// Reset the raw file iterator.</span>
    <span class="n">file</span><span class="p">.</span><span class="py">iter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

    <span class="c">// Decompress the file.</span>
    <span class="k">var</span> <span class="n">decompressed</span> <span class="o">=</span> <span class="k">try</span> <span class="n">decompressFile</span><span class="p">(</span><span class="n">allocator</span><span class="p">,</span> <span class="s">"src/data/ucd/Decompositions.bin"</span><span class="p">);</span>
    <span class="k">defer</span> <span class="n">decompressed</span><span class="p">.</span><span class="nf">deinit</span><span class="p">();</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">file</span><span class="p">.</span><span class="nf">next</span><span class="p">())</span> <span class="p">|</span><span class="n">expected</span><span class="p">|</span> <span class="p">{</span>
        <span class="k">var</span> <span class="n">actual</span> <span class="o">=</span> <span class="n">decompressed</span><span class="p">.</span><span class="nf">next</span><span class="p">()</span><span class="o">.?</span><span class="p">;</span>
        <span class="k">try</span> <span class="n">testing</span><span class="p">.</span><span class="nf">expectEqual</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="a-stream-of-op-codes">A stream of op codes</h3>

<p>I settled on a really simple idea: these data files all have basically just a variable number of integers per line. And if I kept “registers” representing the current value for each integer, I could determine the difference between the past line and the subsequent one to produce a difference. If I encoded that difference as a stream of opcodes with associative data, then to decompress the file I could simply “replay” those operations based on the opcodes and then iteratively come up with more finely-specified, specific opcodes to handle specific types of data.</p>

<p>I started out simple, really just with two opcodes:</p>

<pre><code class="language-Zig">// A UDDC opcode for a decomposition file.
const Opcode = enum(u4) {
    // Sets all the register values with no compression.
    set,

    // Denotes the end of the opcode stream. This is so that we don't need to encode the total
    // number of opcodes in the stream up front (note also the file is bit packed: there may be
    // a few remaining zero bits at the end as padding so we need an EOF opcode rather than say
    // catching the actual file read EOF.)
    eof,
};
</code></pre>

<p>Using these two opcodes, I was able to effectively encode the entire file. The <code class="language-plaintext highlighter-rouge">set</code> opcode had some associative data which effectively expressed an entire raw, uncompressed entry in the file (one line.) This increased the file size since it was effectively just adding 4 bits (the opcode) as additional overhead.</p>

<h3 id="iteratively-finding-the-most-lucrative-opcodes">Iteratively finding the most lucrative opcodes</h3>

<p>To find the most lucrative (i.e. compressed) opcodes, I printed the data I would associate with an opcode (like <code class="language-plaintext highlighter-rouge">set</code>) and then looked for repetitions. Sometimes manually, and sometimes by e.g. piping data to a combination of <code class="language-plaintext highlighter-rouge">sort|uniq -c|sort -r</code> to find common patterns.</p>

<p>Since I was printing <em>differences</em> between e.g. the current value and previous value, it was really easy to find common patterns that appeared in the file very frequently, such as specific fields incrementing by specific amounts with one field being arbitrary:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">// increments key[3] += 1; sets value.seq[0]; emits an entry.</span>
    <span class="c">// 1685 instances</span>
    <span class="n">increment_key_3_and_set_value_seq_0_and_emit</span><span class="p">,</span>
</code></pre></div></div>

<p>Once I had narrowed down to a larger group of opcodes that more specifically represented the data, I was able to print the number of bits required to store the change in specific fields (like <code class="language-plaintext highlighter-rouge">value.seq[0]</code>) and add even more specific opcodes to use variable bit widths:</p>

<div class="language-zig highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">// increments key[3] += 1; sets value.seq[0]; emits an entry.</span>
    <span class="c">// 1685 instances</span>
    <span class="n">increment_key_3_and_set_value_seq_0_2bit_and_emit</span><span class="p">,</span> <span class="c">// 978 instances, 2323 byte reduction</span>
    <span class="n">increment_key_3_and_set_value_seq_0_8bit_and_emit</span><span class="p">,</span> <span class="c">// 269 instances, 437 byte reduction</span>
    <span class="n">increment_key_3_and_set_value_seq_0_21bit_and_emit</span><span class="p">,</span> <span class="c">// 438 instances</span>
</code></pre></div></div>

<p>It being a stream of opcodes was quite nice, because it allowed me to determine how much space was being consumed by a given opcode in sum and target further reducing the size of opcodes that took up the most space. It also made it really easy to find opcodes that I though <em>might</em> help, but in practice turned out to not be that frequent. Just print them, pipe to <code class="language-plaintext highlighter-rouge">sort|uniq -c|sort -r</code> to count them - and remove the lowest hanging fruit.</p>

<h3 id="a-stream-of-opcodes-for-a-state-machine-a-natural-progression-from-a-binary-format">A stream of opcodes for a state machine: a natural progression from a binary format?</h3>

<p>I chose an opcode stream for a reason: so that I could encode some complex logic in the form of a state machine. This came in handy for the <code class="language-plaintext highlighter-rouge">allkeys.txt</code> file in specific, as it allowed me to introduce <em>incrementors</em> into the mix which would <em>increment register values by a chosen amount each iteration (value “emission”)</em>.</p>

<p>The final opcodes for the allkeys.txt file ended up being:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// A UDDC opcode for an allkeys file.
const Opcode = enum(u3) {
    // Sets an incrementor for the key register, incrementing the key by this much on each emission.
    // 10690 instances, 13,480.5 bytes
    inc_key,

    // Sets an incrementor for the value register, incrementing the value by this much on each emission.
    // 7668 instances, 62,970 bytes
    inc_value,

    // Emits a single value.
    // 31001 instances, 15,500.5 bytes
    emit_1,
    emit_2,
    emit_4,
    emit_8,
    emit_32,

    // Denotes the end of the opcode stream. This is so that we don't need to encode the total
    // number of opcodes in the stream up front (note also the file is bit packed: there may be
    // a few remaining zero bits at the end as padding so we need an EOF opcode rather than say
    // catching the actual file read EOF.)
    eof,
};
</code></pre></div></div>

<p>This meant I could determine the difference in the <code class="language-plaintext highlighter-rouge">key</code> and <code class="language-plaintext highlighter-rouge">value</code> fields (what those actually are isn’t important, just that they are all minor incremental differences on the prior entry in the file) - set an <em>incrementor</em> to do some work on each emission, such as say increment the <code class="language-plaintext highlighter-rouge">key</code> array by <code class="language-plaintext highlighter-rouge">[0, 1, 5]</code> each emission, and then say “now emit_32 values!”.</p>

<p>Suddenly, instead of encoding 32 key entries (32 * 3 key values * 21 bits) I am just setting an incrementor (3 key values * 21 bits) and a single opcode to emit 32 values (3 bits).</p>

<p>Overall, this gave me a very nice, natural-feeling progression from a “raw binary format” to something a bit more specific - a bit more <em>compressed.</em></p>

<h2 id="results-better-than-gzipbrotli-and-even-better-with-them">Results? Better than gzip/brotli; and even better <em>with</em> them!</h2>

<p>For lack of better words, I’ll call my compression algorithm here Unicode Data Differential Compression, since it’s differential and specifically for the Unicode data table files - or UDDC for short.</p>

<p>The two files went from the original 568K (with gzip) down to just 61K (with UDDC+gzip). With this, we are able to equal or match both <code class="language-plaintext highlighter-rouge">gzip -9</code> and <code class="language-plaintext highlighter-rouge">brotli -9</code> on their own, AND when combined with gzip or brotli we are able to reduce by 40-70%:</p>

<table>
  <thead>
    <tr>
      <th>File</th>
      <th>Before (bytes)</th>
      <th>After (bytes)</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Decompositions.bin</code></td>
      <td>48,242</td>
      <td>19,072</td>
      <td>-60.5% (-29,170 bytes)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Decompositions.bin.br</code></td>
      <td>24,411</td>
      <td>14,783</td>
      <td>-39.4% (-9,628 bytes)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Decompositions.bin.gz</code></td>
      <td>30,931</td>
      <td>15,670</td>
      <td>-49.34% (15,261 bytes)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">allkeys.bin</code></td>
      <td>373,719</td>
      <td>100,907</td>
      <td>-73.0% (-272,812 bytes)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">allkeys.bin.br</code></td>
      <td>108,982</td>
      <td>44,860</td>
      <td>-58.8% (-64,122 bytes)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">allkeys.bin.gz</code></td>
      <td>163,237</td>
      <td>46,996</td>
      <td>-71.2% (-116,241 bytes)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Before represents binary format without UDDC compression.</li>
  <li>After represents binary format with UDDC compression.</li>
  <li><code class="language-plaintext highlighter-rouge">.br</code> represents <code class="language-plaintext highlighter-rouge">brotli -9 &lt;file&gt;</code> compression</li>
  <li><code class="language-plaintext highlighter-rouge">.gz</code> represents <code class="language-plaintext highlighter-rouge">gzip -9 &lt;file&gt;</code> compression</li>
</ul>

<h3 id="why-test-with-gzipbrotli-but-not-others">Why test with gzip/brotli but not others?</h3>

<p>I chose to compare against gzip/brotli specifically because you get those effectively for free in WebAssembly: browsers already know how to decompress those and ship with gzip/brotli decompressors - so you can use them for free without shipping any additional code.</p>

<h3 id="how-complex-is-the-implementation">How complex is the implementation?</h3>

<p>The final implementation for both files is only a few hundred lines (excluding blank lines, comments, and tests):</p>

<ul>
  <li><a href="https://github.com/jecolon/ziglyph/blob/main/src/collator/AllKeysFile.zig"><code class="language-plaintext highlighter-rouge">AllKeysFile.zig</code></a>: 298 lines</li>
  <li><a href="https://github.com/jecolon/ziglyph/blob/main/src/normalizer/DecompFile.zig"><code class="language-plaintext highlighter-rouge">DecompFile.zig</code></a> 336 lines</li>
</ul>

<p>I have not measured produced machine code size yet, but suspect it is relatively negligible compared to the gains.</p>

<h2 id="notable-mention">Notable mention</h2>

<p>I should mention that the Unicode spec, as <a href="https://github.com/jecolon">@jecolon</a> pointed out to me, does suggest ways to reduce sort key lengths and implement Run-length Compression:</p>

<ul>
  <li>https://unicode.org/reports/tr10/#Reducing_Sort_Key_Lengths</li>
  <li>https://unicode.org/reports/tr10/#Run-length_Compression</li>
</ul>

<p>I wasn’t able to locate an implementation of this (I’d be curious to compare results!) but suspect that, as the run-length compression does not fit the data as tightly, it would not compress quite as well (although would handle any major changes to the type of data in the files without requiring compression algorithm changes better.)</p>

<p>Also of note is that their algorithm only seems to be mentioned in the context of allkeys.txt / the Unicode Collation Algorithm, not in the context of normalization/decompositions from <code class="language-plaintext highlighter-rouge">UnicodeData.txt</code>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Ask questions, stay curious, don’t be afraid to experiment even if it’s outside of your domain of expertise. You might surprise yourself and find something interesting, challenging, and worthwhile.</p>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      <a alt="Twitter" href="https://twitter.com/slimsag""><img src="https://shields.io/badge/Twitter-follow-blue?logo=Twitter" class="color"></a>
&nbsp;
<a alt="RSS Follow" href="/feed.xml""><img src="https://shields.io/badge/RSS-follow-green?logo=RSS" class="color"></a>
&nbsp;
<a alt="License: CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0"><img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg" class="color"></a>
&nbsp;
<a alt="Hexops on GitHub" href="https://github.com/hexops" class="github color" style="background-image: url(https://raw.githubusercontent.com/simple-icons/simple-icons/develop/icons/github.svg);line-height: 1;padding-left: 1.5rem;background-repeat: no-repeat;padding-top: .25rem;"> GitHub</a>
<br>
<br>
<a href="https://hexops.com"><img class="logo color" height="50px" alt="Hexops logo" src="https://raw.githubusercontent.com/hexops/media/main/logo.svg"></a>
<br>


&copy;  2021 Hexops
<link rel="me" href="mailto:support@hexops.com">
    </p>

  </div>

</footer>


  </body>

</html>
